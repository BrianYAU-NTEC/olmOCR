{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e4258c081a41c89a71f74ed03a367f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     77\u001b[39m output_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_page_3_output.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_filename, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(text_output)\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Molmo and PixMo:\\\\nOpen Weights and Open Data\\\\nfor State-of-the']\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: write() argument must be str, not Tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import base64\n",
    "import urllib.request\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "from olmocr.data.renderpdf import render_pdf_to_base64png\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "\n",
    "# Initialize the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\"\"\"\n",
    "# Grab a sample PDF\n",
    "urllib.request.urlretrieve(\"https://molmo.allenai.org/paper.pdf\", \"./paper.pdf\")\n",
    "# Render page 1 to an image\n",
    "image_base64 = render_pdf_to_base64png(\"./paper.pdf\", 1, target_longest_image_dim=1024)\n",
    "# Build the prompt, using document metadata\n",
    "anchor_text = get_anchor_text(\"./paper.pdf\", 1, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "prompt = build_finetuning_prompt(anchor_text)\n",
    "\"\"\"\n",
    "\n",
    "# Grab a sample PDF\n",
    "pdf_path = \"./SASA.pdf\"\n",
    "# Render page 1 to an image\n",
    "image_base64 = render_pdf_to_base64png(\"./SASA.pdf\", 3, target_longest_image_dim=1024)\n",
    "# Build the prompt, using document metadata\n",
    "anchor_text = get_anchor_text(\"./SASA.pdf\", 1, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "prompt = build_finetuning_prompt(anchor_text)\n",
    "\n",
    "# Build the full prompt\n",
    "messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "# Apply the chat template and processor\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "main_image = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=[main_image],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "\n",
    "# Generate the output\n",
    "output = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.8,\n",
    "            max_new_tokens=50,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "# Decode the output\n",
    "prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "new_tokens = output[:, prompt_length:]\n",
    "text_output = processor.tokenizer.batch_decode(\n",
    "    new_tokens, skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Save the output to a new text file with the PDF name appended with the page number\n",
    "output_filename = f\"{pdf_path.split('/')[-1].split('.')[0]}_page_3_output.txt\"\n",
    "with open(output_filename, 'w') as f:\n",
    "    f.write(output[0])\n",
    "\n",
    "print(text_output)\n",
    "# ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Molmo and PixMo:\\\\nOpen Weights and Open Data\\\\nfor State-of-the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602dc059f7b74efa88e19f018874111d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./source/SASA.pdf\n",
      "Output for SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item | Vendor Article No | Article No | Article Description | Article Status | Color | Item']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import base64\n",
    "import urllib.request\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "from olmocr.data.renderpdf import render_pdf_to_base64png\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "\n",
    "# Initialize the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Specify the source folder containing PDF files\n",
    "source_folder = \"./source\"\n",
    "\n",
    "# Iterate through all PDF files in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith(\".pdf\"):  # Process only PDF files\n",
    "        pdf_path = os.path.join(source_folder, filename)\n",
    "        print(f\"Processing file: {pdf_path}\")\n",
    "        \n",
    "        # Render the first page of the PDF to a base64-encoded PNG image\n",
    "        try:\n",
    "            image_base64 = render_pdf_to_base64png(pdf_path, 1, target_longest_image_dim=1024)\n",
    "        except Exception as e:\n",
    "            print(f\"Error rendering PDF {filename}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract anchor text from the first page of the PDF\n",
    "        try:\n",
    "            anchor_text = get_anchor_text(pdf_path, 1, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting anchor text from {filename}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Build the prompt using document metadata\n",
    "        prompt = build_finetuning_prompt(anchor_text)\n",
    "        \n",
    "        # Build the full prompt with text and image content\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Apply the chat template and processor\n",
    "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Decode the base64 image to a PIL Image object\n",
    "        try:\n",
    "            main_image = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding image for {filename}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare inputs for the model\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=[main_image],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "        \n",
    "        # Generate output from the model\n",
    "        try:\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                temperature=0.8,\n",
    "                max_new_tokens=50,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "            )\n",
    "            \n",
    "            # Decode the output tokens into text\n",
    "            prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "            new_tokens = output[:, prompt_length:]\n",
    "            text_output = processor.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            print(f\"Output for {filename}: {text_output}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating output for {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143eab964e95406db64605f8631f74f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./source/SASA.pdf\n",
      "Processing page 1 of SASA.pdf\n",
      "Output for page 1 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item | Vendor Article No | Article No | Article Status | Color | Item category | Size']\n",
      "Processing page 2 of SASA.pdf\n",
      "Output for page 2 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item Description | Article No | Article Status | Color | Item category | Size | Delivery Date']\n",
      "Processing page 3 of SASA.pdf\n",
      "Output for page 3 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Purchase Order\\\\n\\\\nPO Number : 45028888160']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import base64\n",
    "import urllib.request\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "\n",
    "# Initialize the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Specify the source folder containing PDF files\n",
    "source_folder = \"./source\"\n",
    "\n",
    "# Iterate through all PDF files in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith(\".pdf\"):  # Process only PDF files\n",
    "        pdf_path = os.path.join(source_folder, filename)\n",
    "        print(f\"Processing file: {pdf_path}\")\n",
    "        \n",
    "        # Determine the number of pages in the PDF\n",
    "        pdf_reader = PdfReader(pdf_path)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        # Process each page of the PDF\n",
    "        for page_num in range(1, num_pages + 1):\n",
    "            print(f\"Processing page {page_num} of {filename}\")\n",
    "            \n",
    "            # Render the current page to an image\n",
    "            images = convert_from_path(pdf_path)\n",
    "            image = images[page_num - 1]  # Adjust for 0-based indexing\n",
    "            \n",
    "            # Optionally resize the image to fit within target dimensions\n",
    "            width, height = image.size\n",
    "            target_longest_image_dim = 1024\n",
    "            ratio = min(target_longest_image_dim / width, target_longest_image_dim / height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            image = image.resize(new_size)\n",
    "            \n",
    "            # Convert the image to base64\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            image_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "            \n",
    "            # Extract anchor text from the current page of the PDF\n",
    "            try:\n",
    "                anchor_text = get_anchor_text(pdf_path, page_num, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting anchor text from page {page_num} of {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Build the prompt using document metadata\n",
    "            prompt = build_finetuning_prompt(anchor_text)\n",
    "            \n",
    "            # Build the full prompt with text and image content\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Apply the chat template and processor\n",
    "            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            \n",
    "            # Decode the base64 image to a PIL Image object\n",
    "            try:\n",
    "                main_image = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
    "            except Exception as e:\n",
    "                print(f\"Error decoding image for page {page_num} of {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare inputs for the model\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=[main_image],\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "            \n",
    "            # Generate output from the model\n",
    "            try:\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    temperature=0.8,\n",
    "                    max_new_tokens=50,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "                \n",
    "                # Decode the output tokens into text\n",
    "                prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "                new_tokens = output[:, prompt_length:]\n",
    "                text_output = processor.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "                \n",
    "                print(f\"Output for page {page_num} of {filename}: {text_output}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating output for page {page_num} of {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3907975741934543944b9e348425a54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./source/SASA.pdf\n",
      "Processing page 1 of SASA.pdf\n",
      "Output for page 1 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item | Vendor Article No | Article No | Article Status | Color | Item category | Size']\n",
      "Processing page 2 of SASA.pdf\n",
      "Output for page 2 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item Description | Article No | Article Status | Color | Item category | Size | Delivery Date']\n",
      "Processing page 3 of SASA.pdf\n",
      "Output for page 3 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Terms and Conditions (T&C): Unless otherwise agreed in writing between the parties, these']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import base64\n",
    "import urllib.request\n",
    "import camelot\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "\n",
    "# Initialize the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Specify the source folder containing PDF files\n",
    "source_folder = \"./source\"\n",
    "\n",
    "# Iterate through all PDF files in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith(\".pdf\"):  # Process only PDF files\n",
    "        pdf_path = os.path.join(source_folder, filename)\n",
    "        print(f\"Processing file: {pdf_path}\")\n",
    "        \n",
    "        # Determine the number of pages in the PDF\n",
    "        pdf_reader = PdfReader(pdf_path)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        # Process each page of the PDF\n",
    "        for page_num in range(1, num_pages + 1):\n",
    "            print(f\"Processing page {page_num} of {filename}\")\n",
    "            \n",
    "            # Render the current page to an image\n",
    "            images = convert_from_path(pdf_path)\n",
    "            image = images[page_num - 1]  # Adjust for 0-based indexing\n",
    "            \n",
    "            # Optionally resize the image to fit within target dimensions\n",
    "            width, height = image.size\n",
    "            target_longest_image_dim = 1024\n",
    "            ratio = min(target_longest_image_dim / width, target_longest_image_dim / height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            image = image.resize(new_size)\n",
    "            \n",
    "            # Convert the image to base64\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            image_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "            \n",
    "            # Extract anchor text from the current page of the PDF\n",
    "            try:\n",
    "                anchor_text = get_anchor_text(pdf_path, page_num, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting anchor text from page {page_num} of {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Build the prompt using document metadata\n",
    "            prompt = build_finetuning_prompt(anchor_text)\n",
    "            \n",
    "            # Build the full prompt with text and image content\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Apply the chat template and processor\n",
    "            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            \n",
    "            # Decode the base64 image to a PIL Image object\n",
    "            try:\n",
    "                main_image = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
    "            except Exception as e:\n",
    "                print(f\"Error decoding image for page {page_num} of {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare inputs for the model\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=[main_image],\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "            \n",
    "            # Generate output from the model\n",
    "            try:\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    temperature=0.8,\n",
    "                    max_new_tokens=50,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "                \n",
    "                # Decode the output tokens into text\n",
    "                prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "                new_tokens = output[:, prompt_length:]\n",
    "                text_output = processor.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "                \n",
    "                print(f\"Output for page {page_num} of {filename}: {text_output}\")\n",
    "                \n",
    "                # Check if the output indicates a table\n",
    "                if '\"is_table\": true' in str(text_output):\n",
    "                    # Extract tables using camelot\n",
    "                    tables = camelot.read_pdf(pdf_path, pages=str(page_num))\n",
    "                    \n",
    "                    # Export the first table to CSV\n",
    "                    if tables:\n",
    "                        tables[0].to_csv(f\"table_page_{page_num}.csv\")\n",
    "                        print(f\"Table extracted from page {page_num} and saved to CSV.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating output for page {page_num} of {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34c29768f0e408daaffb7d99ef8643d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./source/SASA.pdf\n",
      "Processing page 1 of SASA.pdf\n",
      "Output for page 1 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item | Vendor Article No | Article No | Article Status | Color | Item category | Size']\n",
      "Processing page 2 of SASA.pdf\n",
      "Output for page 2 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item Description | Article No | Article Status | Color | Item category | Size | Delivery Date']\n",
      "Processing page 3 of SASA.pdf\n",
      "Output for page 3 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Purchase Order\\\\n\\\\nPO Number : 45028888160']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import base64\n",
    "import urllib.request\n",
    "import camelot\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "\n",
    "# Initialize the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Specify the source folder containing PDF files\n",
    "source_folder = \"./source\"\n",
    "\n",
    "# Iterate through all PDF files in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith(\".pdf\"):  # Process only PDF files\n",
    "        pdf_path = os.path.join(source_folder, filename)\n",
    "        print(f\"Processing file: {pdf_path}\")\n",
    "        \n",
    "        # Determine the number of pages in the PDF\n",
    "        pdf_reader = PdfReader(pdf_path)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        # Process each page of the PDF\n",
    "        for page_num in range(1, num_pages + 1):\n",
    "            print(f\"Processing page {page_num} of {filename}\")\n",
    "            \n",
    "            # Render the current page to an image\n",
    "            images = convert_from_path(pdf_path)\n",
    "            image = images[page_num - 1]  # Adjust for 0-based indexing\n",
    "            \n",
    "            # Optionally resize the image to fit within target dimensions\n",
    "            width, height = image.size\n",
    "            target_longest_image_dim = 1024\n",
    "            ratio = min(target_longest_image_dim / width, target_longest_image_dim / height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            image = image.resize(new_size)\n",
    "            \n",
    "            # Convert the image to base64\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            image_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "            \n",
    "            # Extract anchor text from the current page of the PDF\n",
    "            try:\n",
    "                anchor_text = get_anchor_text(pdf_path, page_num, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting anchor text from page {page_num} of {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Build the prompt using document metadata\n",
    "            prompt = build_finetuning_prompt(anchor_text)\n",
    "            \n",
    "            # Build the full prompt with text and image content\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Apply the chat template and processor\n",
    "            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            \n",
    "            # Decode the base64 image to a PIL Image object\n",
    "            try:\n",
    "                main_image = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
    "            except Exception as e:\n",
    "                print(f\"Error decoding image for page {page_num} of {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare inputs for the model\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=[main_image],\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "            \n",
    "            # Generate output from the model\n",
    "            try:\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    temperature=0.8,\n",
    "                    max_new_tokens=50,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "                \n",
    "                # Decode the output tokens into text\n",
    "                prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "                new_tokens = output[:, prompt_length:]\n",
    "                text_output = processor.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "                \n",
    "                print(f\"Output for page {page_num} of {filename}: {text_output}\")\n",
    "                \n",
    "                # Check if the output indicates a table\n",
    "                if '\"is_table\": true' in str(text_output):\n",
    "                    # Extract tables using camelot\n",
    "                    try:\n",
    "                        tables = camelot.read_pdf(pdf_path, pages=str(page_num))\n",
    "                        \n",
    "                        # Export the first table to CSV\n",
    "                        if tables:\n",
    "                            tables[0].to_csv(f\"table_page_{page_num}.csv\")\n",
    "                            print(f\"Table extracted from page {page_num} and saved to CSV.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting table from page {page_num} of {filename}: {e}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating output for page {page_num} of {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781841f04d57478299c1d26d51e5eb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./source/SASA.pdf\n",
      "Processing page 1 of SASA.pdf\n",
      "Output for page 1 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item | Vendor Article No | Article No | Article Status | Color | Item category | Size']\n",
      "Error parsing natural text from page 1 of SASA.pdf. Using full output text instead.\n",
      "Processing page 2 of SASA.pdf\n",
      "Output for page 2 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item Description | Article No | Article Status | Color | Item category | Size | Delivery Date']\n",
      "Error parsing natural text from page 2 of SASA.pdf. Using full output text instead.\n",
      "Processing page 3 of SASA.pdf\n",
      "Output for page 3 of SASA.pdf: ['{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Terms and Conditions (T&C): Unless otherwise agreed in writing between the parties, these']\n",
      "Error parsing natural text from page 3 of SASA.pdf. Using full output text instead.\n",
      "All natural text from SASA.pdf:\n",
      "Page 1: {\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item | Vendor Article No | Article No | Article Status | Color | Item category | Size\n",
      "Page 2: {\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item Description | Article No | Article Status | Color | Item category | Size | Delivery Date\n",
      "Page 3: {\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Terms and Conditions (T&C): Unless otherwise agreed in writing between the parties, these\n",
      "Natural text saved to SASA_natural_text.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import base64\n",
    "import urllib.request\n",
    "import camelot\n",
    "import re  # Import the re module for regular expressions\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "\n",
    "# Initialize the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Specify the source folder containing PDF files\n",
    "source_folder = \"./source\"\n",
    "\n",
    "# Iterate through all PDF files in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith(\".pdf\"):  # Process only PDF files\n",
    "        pdf_path = os.path.join(source_folder, filename)\n",
    "        print(f\"Processing file: {pdf_path}\")\n",
    "        \n",
    "        # Determine the number of pages in the PDF\n",
    "        pdf_reader = PdfReader(pdf_path)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        # Initialize a list to store natural text from all pages\n",
    "        all_natural_text = []\n",
    "        \n",
    "        # Process each page of the PDF\n",
    "        for page_num in range(1, num_pages + 1):\n",
    "            print(f\"Processing page {page_num} of {filename}\")\n",
    "            \n",
    "            # Render the current page to an image\n",
    "            images = convert_from_path(pdf_path)\n",
    "            image = images[page_num - 1]  # Adjust for 0-based indexing\n",
    "            \n",
    "            # Optionally resize the image to fit within target dimensions\n",
    "            width, height = image.size\n",
    "            target_longest_image_dim = 1024\n",
    "            ratio = min(target_longest_image_dim / width, target_longest_image_dim / height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            image = image.resize(new_size)\n",
    "            \n",
    "            # Convert the image to base64\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            image_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "            \n",
    "            # Extract anchor text from the current page of the PDF\n",
    "            try:\n",
    "                anchor_text = get_anchor_text(pdf_path, page_num, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting anchor text from page {page_num} of {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Build the prompt using document metadata\n",
    "            prompt = build_finetuning_prompt(anchor_text)\n",
    "            \n",
    "            # Build the full prompt with text and image content\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Apply the chat template and processor\n",
    "            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            \n",
    "            # Decode the base64 image to a PIL Image object\n",
    "            try:\n",
    "                main_image = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
    "            except Exception as e:\n",
    "                print(f\"Error decoding image for page {page_num} of {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare inputs for the model\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=[main_image],\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "            \n",
    "            # Generate output from the model\n",
    "            try:\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    temperature=0.8,\n",
    "                    max_new_tokens=50,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "                \n",
    "                # Decode the output tokens into text\n",
    "                prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "                new_tokens = output[:, prompt_length:]\n",
    "                text_output = processor.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "                \n",
    "                print(f\"Output for page {page_num} of {filename}: {text_output}\")\n",
    "                \n",
    "                # Manually extract natural text from the output\n",
    "                for output_text in text_output:\n",
    "                    if output_text:\n",
    "                        match = re.search(r'\"natural_text\":\"([^\"]*)\"', output_text)\n",
    "                        if match:\n",
    "                            natural_text = match.group(1)\n",
    "                            all_natural_text.append(natural_text)\n",
    "                        else:\n",
    "                            # If parsing fails, append the full output text as a fallback\n",
    "                            all_natural_text.append(output_text)\n",
    "                            print(f\"Error parsing natural text from page {page_num} of {filename}. Using full output text instead.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating output for page {page_num} of {filename}: {e}\")\n",
    "        \n",
    "        # Save or print all natural text\n",
    "        print(f\"All natural text from {filename}:\")\n",
    "        for i, natural_text in enumerate(all_natural_text):\n",
    "            print(f\"Page {i+1}: {natural_text}\")\n",
    "        \n",
    "        # Optionally save all natural text to a file\n",
    "        with open(f\"{os.path.splitext(filename)[0]}_natural_text.txt\", \"w\") as f:\n",
    "            for natural_text in all_natural_text:\n",
    "                f.write(natural_text + \"\\n\\n\")\n",
    "        print(f\"Natural text saved to {os.path.splitext(filename)[0]}_natural_text.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmOCR311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
